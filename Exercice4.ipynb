{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e24d7-162a-49ec-8a27-58099d076179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
    "# Please note that the weight of the grade for the question is relative\n",
    "# to its difficulty. So your Category 1 question will score significantly\n",
    "# less than your Category 5 question.\n",
    "#\n",
    "# Don't use lambda layers in your model.\n",
    "# You do not need them to solve the question.\n",
    "# Lambda layers are not supported by the grading infrastructure.\n",
    "#\n",
    "# You must use the Submit and Test button to submit your model\n",
    "# at least once in this category before you finally submit your exam,\n",
    "# otherwise you will score zero for this category.\n",
    "# ======================================================================\n",
    "#\n",
    "# NLP QUESTION\n",
    "#\n",
    "# Build and train a classifier for the sarcasm dataset.\n",
    "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\n",
    "# It will be tested against a number of sentences that the network hasn't previously seen\n",
    "# and you will be scored on whether sarcasm was correctly detected in those sentences.\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def solution_model():\n",
    "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
    "\n",
    "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
    "    vocab_size = 10000\n",
    "    embedding_dim = 1000\n",
    "    max_length = 120\n",
    "    trunc_type='post'\n",
    "    padding_type='post'\n",
    "    oov_tok = \"<OOV>\"\n",
    "    training_size = 20000\n",
    "\n",
    "    # Load the JSON data\n",
    "    with open('sarcasm.json', 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for item in datastore:\n",
    "        sentences.append(item['headline'])\n",
    "        labels.append(item['is_sarcastic'])\n",
    "\n",
    "    # Tokenize and pad sequences\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    padded = np.array(padded)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(2),  # Ajout d'une couche de pooling\n",
    "        tf.keras.layers.Conv1D(32, 5, activation='relu'),  # RÃ©duction du nombre de filtres\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),  # Augmentation du nombre de neurones\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    train_size = int(len(sentences) * 0.8)\n",
    "    train_sentences = padded[:train_size]\n",
    "    val_sentences = padded[train_size:]\n",
    "    train_labels = labels[:train_size]\n",
    "    val_labels = labels[train_size:]\n",
    "\n",
    "    model.fit(train_sentences, train_labels, epochs=10, validation_data=(val_sentences, val_labels))\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = solution_model()\n",
    "    model.save(\"mymodel.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
